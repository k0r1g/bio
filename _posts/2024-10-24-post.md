---
layout: post
title: "Humanity's Final Invention"
date: 2024-10-24
categories: blog
author: Kori Rogers
tags: none
---
Humanity's final invention is a universe simulator. A so-called [Turing Oracle](https://en.wikipedia.org/wiki/Oracle_machine).

Truly solving simulations = building the final technology of the universe.

If you've built a universe simulator, you have answered the question: "is the universe deterministic?".

Proving Nick Bolstrom's [Simulation Hypothesis](https://en.wikipedia.org/wiki/Simulation_hypothesis) would simply be a matter of pressing 'run' on your universe simulator.

By inventing the universe simulator, you have invented all possible technology. 

Some people tend to call text-to-video models ['World Models'](https://openai.com/index/video-generation-models-as-world-simulators/). These diffusion-based transformer models appear to implicitely learn physics (without ontology) by gradient descent through massive amounts of video data. The most optimistic interpretation of the results we've seen from video models is to say: "machines can learn the physics of the universe simply by observing it". In other words, a data-driven physics engine.

Perhaps the universe simulator is simply what we've been calling 'Artificial Intelligence' all along, just at the asymptote of scaling. After all, don't large language models simply simulate the next token. After extrapolating on multi-modality, mathematical capability, and some of what we've seen with video models, don't simulations seem like a natural extension? If the [Scaling Laws](https://arxiv.org/abs/2001.08361) continue to hold, then in *x-thousand years*, humanity's greatest achievement will be to build as many sensors as possible at every possible scale of measurement, to build towards an [*infinite tape of data*](https://en.wikipedia.org/wiki/Turing_completeness) that is fed into an ultimate simulation engine which will predict the next state of the universe.

We see shades of this in the world today. 

Rather than multiple models to trade, some hypothesise that Reneissance Technologies work on a single very large model that pulls from incredibly diverse sources of data. As the hedge fund with the best track record on Wall Street, isn't this a (albeit imperfect) simulator, verticalised in financial markets, on very short time scales? 

Or Chile's [Project Cybersyn](https://en.wikipedia.org/wiki/Project_Cybersyn), which aimed to build an economic simulator to forecast the Chilean economy and guide policy. 

The way physics (e.g. climate modelling or modelling the fluid dynamics on a rocket etc.) is simulated today is though building differential equations that represent the system, and because often these systems of equations cannot be solved analytically, numeric methods are used to solve them- often on large supercomputers. By using machine learning models as general function approximators, we're now seeing them used as computationally-cheaper alternatives to these numeric methods.

Learn from the [bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), progressively build towards an infinite tape of data, rely on machine learning's ability to be a general function approximator. At the limit, we may discover the function of the universe and solve the halting problem. Build the universe simulator. This will be mankind's final invention. 

[back]({{ site.url }})